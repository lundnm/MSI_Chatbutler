import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv
import warnings
warnings.filterwarnings("ignore")

load_dotenv(dotenv_path='.env')
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


class ChatButler:
    """
    This class initializes a conversation bot that:
      1. Loads the specified PDF.
      2. Creates a vector store for retrieval.
      3. Uses ConversationalRetrievalChain to handle multi-turn Q&A.
      4. Stores conversation history with ConversationBufferMemory
         so that the bot can reference past user inputs and responses.
    """

    def __init__(self):
        """
        Initialize the ChatButler class.
        - Set the OpenAI API base URL and key (for vocareum environment, if needed).
        - Specify the chat model and openai_api_key.
        - Load the PDF into documents.
        - Create an embeddings object.
        - Create a vector store (Chroma) from the documents and embeddings.
        - Initialize a memory object to keep track of the conversation history.
        - Build a ConversationalRetrievalChain with that memory and vector store.
        - Setup a system prompt to provide context and guidelines to the AI assistant.
        """

        self.api_base = "https://openai.vocareum.com/v1"
        self.api_key = OPENAI_API_KEY
        
        self.llm = ChatOpenAI(
            model="gpt-4o-2024-08-06",
            openai_api_key=self.api_key,
            openai_api_base=self.api_base
        )
        pdf_dir_path = "data/"

        # List to hold all documents
        all_documents = []

        # Iterate through all files in the directory
        for filename in os.listdir(pdf_dir_path):
            # Check if the file is a PDF
            if filename.endswith('.pdf'):
                # Full path to the PDF file
                pdf_path = os.path.join(pdf_dir_path, filename)
        
                # Load the PDF
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
        
                # Append the loaded documents to the list
                all_documents.extend(documents)     
        
        
        # # Example PDF file
        # pdf_path = "data/"
        # loader = PyPDFLoader(pdf_path)
        # documents = loader.load()

        embeddings = OpenAIEmbeddings(openai_api_key=self.api_key, openai_api_base=self.api_base)
        vector_store = Chroma.from_documents(all_documents, embeddings)

        # This memory buffer will keep track of all past user-bot exchanges
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        self.conv_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=vector_store.as_retriever(),
            memory=self.memory
        )

        # A system prompt (role) describing how the assistant should behave
        self.system_role = (
            "You are a helpful assistant designed to help master's students in "
            "data analytics at the University of Michigan School of Information. "
            "Your main role is to provide information about course offerings and "
            "degree requirements using the loaded PDF data. "
            "Maintain a professional tone and ensure accurate, concise answers."
        )

        # Store system messages if desired. This example just keeps track.
        self.messages = [{"role": "system", "content": self.system_role}]

    def chat(self, messages):
        """
        Handle the last user message by passing it into the
        ConversationalRetrievalChain. The chain automatically
        uses the memory to include past conversation history.

        Args:
            messages (List[Dict]): The entire conversation so far,
                                   but we mainly need the last user input.

        Returns:
            str: The assistant's answer generated by the chain.
        """
        user_input = messages[-1]["content"]
        # Invoke the chain: pass the user's question, the memory is implicitly used.
        result = self.conv_chain({"question": user_input})
        # "answer" is typically the key that holds the text output.
        return result["answer"]

    def begin_chat(self):
        """
        Runs a CLI loop that allows a user to input queries.
        Type 'exit' to end the interaction.
        """
        print("\nWelcome to the MSI:BDA Information Retrieval Bot!")
        print("You can ask about course offerings, degree requirements, etc.")
        print("Please include as much information as you feel is necessary for the bot to assist you.")
        print("Type 'exit' to end the chat.\n")

        while True:
            user_input = input("User: ")
            if user_input.lower().strip() == "exit":
                print("Exiting the chat.")
                break

            # Store the user message
            self.messages.append({"role": "user", "content": user_input})

            # Get assistant response by calling our chat method
            system_response = self.chat(self.messages)

            # Store the assistant message
            self.messages.append({"role": "assistant", "content": system_response})

            # Print the assistant response
            print(f"Assistant: {system_response}")


def main():
    chat = ChatButler()
    chat.begin_chat()

if __name__ == "__main__":
    main()